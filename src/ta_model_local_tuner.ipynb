{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff082d54",
   "metadata": {},
   "source": [
    "# Hybrid Autoencoder + CNN ‚Äî Lokal/Colab + Google Drive Sync + Keras Tuner (LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906872b",
   "metadata": {},
   "source": [
    "Notebook singkat, readable, dan siap jalan di lokal atau Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46620a2",
   "metadata": {},
   "source": [
    "## 1) Cek versi & GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a192a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Python     :\", sys.version)\n",
    "print(\"Platform   :\", platform.platform())\n",
    "print(\"TensorFlow :\", tf.__version__)\n",
    "print(\"GPU        :\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56334208",
   "metadata": {},
   "source": [
    "## 2) Konfigurasi data & mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc63655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Pilih sumber data: LOCAL, COLAB_DRIVE, atau GDRIVE_SYNC\n",
    "DATA_SOURCE = \"GDRIVE_SYNC\"\n",
    "\n",
    "# Path untuk setiap sumber data\n",
    "LOCAL_DATA_DIR = r\"./dataset_babybrandedshop\"\n",
    "COLAB_DRIVE_DIR = \"/content/drive/MyDrive/dataset_babybrandedshop\"\n",
    "GDRIVE_FOLDER_ID = \"1dtwyooh7nl5hivsFBGOL3qG6q9HF7xYF\"\n",
    "\n",
    "# Buat folder cache untuk sinkronisasi Google Drive\n",
    "SYNC_CACHE_DIR = Path(\"./gdrive_cache_dataset\")\n",
    "SYNC_CACHE_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20abbd3e",
   "metadata": {},
   "source": [
    "## 3) Utilitas sinkronisasi Google Drive (lokal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25166fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDENTIAL = \"client_secret_376472766929-sa1l06mdun2qkoi3o20vtslr103mkspi.apps.googleusercontent.com.json\"\n",
    "\n",
    "def sync_gdrive_folder(folder_id: str, target_dir: Path):\n",
    "    \"\"\"Download seluruh folder dari Google Drive ke lokal\"\"\"\n",
    "    from pydrive2.auth import GoogleAuth\n",
    "    from pydrive2.drive import GoogleDrive\n",
    "    \n",
    "    # Validasi folder ID\n",
    "    assert folder_id, \"GDRIVE_FOLDER_ID kosong.\"\n",
    "    \n",
    "    # Autentikasi Google Drive\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.LoadCredentialsFile(CREDENTIAL)\n",
    "    \n",
    "    if gauth.credentials is None:\n",
    "        gauth.LocalWebserverAuth()\n",
    "        gauth.SaveCredentialsFile(CREDENTIAL)\n",
    "    elif gauth.access_token_expired:\n",
    "        gauth.Refresh()\n",
    "        gauth.SaveCredentialsFile(CREDENTIAL)\n",
    "    else:\n",
    "        gauth.Authorize()\n",
    "    \n",
    "    drive = GoogleDrive(gauth)\n",
    "    \n",
    "    # Fungsi helper untuk membuat direktori\n",
    "    def ensure_dir(p: Path):\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Fungsi untuk list file/folder dalam folder tertentu\n",
    "    def list_children(fid: str):\n",
    "        query = f\"'{fid}' in parents and trashed=false\"\n",
    "        return drive.ListFile({'q': query}).GetList()\n",
    "    \n",
    "    # Fungsi rekursif untuk download folder dan isinya\n",
    "    def recursive_download(fid: str, dest: Path):\n",
    "        ensure_dir(dest)\n",
    "        for f in list_children(fid):\n",
    "            # Jika folder, download rekursif\n",
    "            if f['mimeType'] == 'application/vnd.google-apps.folder':\n",
    "                recursive_download(f['id'], dest / f['title'])\n",
    "            # Jika file, download langsung\n",
    "            else:\n",
    "                out = dest / f['title']\n",
    "                if not out.exists():\n",
    "                    f.GetContentFile(str(out))\n",
    "    \n",
    "    recursive_download(folder_id, target_dir)\n",
    "    return target_dir\n",
    "\n",
    "print(\"Fungsi sync_gdrive_folder() siap digunakan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd221dd",
   "metadata": {},
   "source": [
    "## 4) Tentukan DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfdc8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek apakah sedang berjalan di Google Colab\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Tentukan DATA_DIR berdasarkan sumber data\n",
    "if DATA_SOURCE == \"LOCAL\":\n",
    "    # Gunakan dataset lokal\n",
    "    DATA_DIR = Path(LOCAL_DATA_DIR).resolve()\n",
    "    \n",
    "elif DATA_SOURCE == \"COLAB_DRIVE\":\n",
    "    # Gunakan Google Drive di Colab\n",
    "    assert IN_COLAB, \"Mode ini hanya untuk Google Colab.\"\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    DATA_DIR = Path(COLAB_DRIVE_DIR).resolve()\n",
    "    \n",
    "elif DATA_SOURCE == \"GDRIVE_SYNC\":\n",
    "    # Sinkronisasi dari Google Drive ke lokal\n",
    "    DATA_DIR = sync_gdrive_folder(GDRIVE_FOLDER_ID, Path(\"./gdrive_cache_dataset\")).resolve()\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"DATA_SOURCE tidak valid. Pilih: LOCAL, COLAB_DRIVE, atau GDRIVE_SYNC\")\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "assert DATA_DIR.exists(), f\"Folder {DATA_DIR} tidak ditemukan!\"\n",
    "print(\"Dataset aman!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee133a8",
   "metadata": {},
   "source": [
    "## 5) Dataset TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950359d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Konfigurasi ukuran gambar dan batch\n",
    "IMG_SIZE = (256, 256)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load dataset training (80% data)\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Load dataset validasi (20% data)\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Optimasi performa dengan prefetch dan cache\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.prefetch(AUTOTUNE).cache()\n",
    "val_ds = val_ds.prefetch(AUTOTUNE).cache()\n",
    "\n",
    "# Deteksi kelas dari folder\n",
    "classes = sorted([p.name for p in Path(DATA_DIR).iterdir() if p.is_dir()])\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "print(f\"Jumlah kelas: {NUM_CLASSES}\")\n",
    "print(f\"Nama kelas: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c1b864",
   "metadata": {},
   "source": [
    "## 6) Augmentasi & Normalisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da224c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Layer augmentasi data (untuk variasi gambar saat training)\n",
    "augment = keras.Sequential([\n",
    "    layers.RandomFlip(mode=\"horizontal_and_vertical\"),  # Flip horizontal & vertikal\n",
    "    layers.RandomRotation(factor=0.0833, fill_mode=\"reflect\"),  # Rotasi ¬±30¬∞\n",
    "    layers.RandomZoom(height_factor=(-0.10, 0.10), width_factor=(-0.10, 0.10)),  # Zoom ¬±10%\n",
    "    layers.RandomContrast(factor=0.2),  # Ubah kontras\n",
    "    layers.RandomBrightness(factor=0.1),  # Ubah kecerahan\n",
    "], name=\"augment\")\n",
    "\n",
    "# Layer preprocessing (resize dan normalisasi)\n",
    "preprocess = keras.Sequential([\n",
    "    layers.Resizing(IMG_SIZE[0], IMG_SIZE[1]),  # Resize ke ukuran yang sama\n",
    "    layers.Rescaling(1./255),  # Normalisasi pixel dari 0-255 ke 0-1\n",
    "], name=\"preprocess\")\n",
    "\n",
    "print(\"Augmentasi dan preprocessing done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c87784",
   "metadata": {},
   "source": [
    "## 7) Bangun Autoencoder (Encoder + Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b47890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow import keras\n",
    "\n",
    "def build_encoder(input_shape=(256, 256, 3), latent_dim=128):\n",
    "    \"\"\"\n",
    "    Bangun Encoder: kompres gambar menjadi representasi fitur (latent vector)\n",
    "    Input: gambar 256x256x3\n",
    "    Output: vector 128 dimensi\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = preprocess(inputs)\n",
    "    \n",
    "    # Layer 1: 256x256x3 -> 128x128x32\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    \n",
    "    # Layer 2: 128x128x32 -> 64x64x64\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    \n",
    "    # Layer 3: 64x64x64 -> 32x32x128\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    \n",
    "    # Layer 4: 32x32x128 -> 16x16x256\n",
    "    x = layers.Conv2D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    \n",
    "    # Flatten dan kompres ke latent vector\n",
    "    x = layers.Flatten()(x)\n",
    "    latent = layers.Dense(latent_dim, activation='relu', name='latent')(x)\n",
    "    \n",
    "    return Model(inputs, latent, name='encoder')\n",
    "\n",
    "\n",
    "def build_decoder(output_shape=(256, 256, 3), latent_dim=128):\n",
    "    \"\"\"\n",
    "    Bangun Decoder: rekonstruksi gambar dari latent vector\n",
    "    Input: vector 128 dimensi\n",
    "    Output: gambar 256x256x3\n",
    "    \"\"\"\n",
    "    H, W, C = output_shape\n",
    "    start_h, start_w, start_c = 16, 16, 256  # Ukuran awal: 16x16x256\n",
    "    \n",
    "    latent_in = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(start_h * start_w * start_c, activation='relu')(latent_in)\n",
    "    x = layers.Reshape((start_h, start_w, start_c))(x)\n",
    "    \n",
    "    # Layer 1: 16x16x256 -> 32x32x256\n",
    "    x = layers.Conv2DTranspose(256, 3, strides=2, activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Layer 2: 32x32x256 -> 64x64x128\n",
    "    x = layers.Conv2DTranspose(128, 3, strides=2, activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Layer 3: 64x64x128 -> 128x128x64\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Layer 4: 128x128x64 -> 256x256x32\n",
    "    x = layers.Conv2DTranspose(32, 3, strides=2, activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Output layer: 256x256x32 -> 256x256x3\n",
    "    outputs = layers.Conv2D(C, 3, activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    return Model(latent_in, outputs, name='decoder')\n",
    "\n",
    "\n",
    "# Bangun encoder dan decoder\n",
    "encoder = build_encoder(IMG_SIZE + (3,), latent_dim=128)\n",
    "decoder = build_decoder(IMG_SIZE + (3,), latent_dim=128)\n",
    "\n",
    "# Gabungkan encoder + decoder menjadi autoencoder\n",
    "inp = keras.Input(shape=IMG_SIZE + (3,))\n",
    "z = encoder(inp)  # Encode gambar\n",
    "recon = decoder(z)  # Decode kembali\n",
    "autoencoder = Model(inp, recon, name='autoencoder')\n",
    "\n",
    "# Compile autoencoder\n",
    "autoencoder.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='mse'  # Mean Squared Error untuk rekonstruksi\n",
    ")\n",
    "\n",
    "print(\"\\nArsitektur Autoencoder:\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88be1948",
   "metadata": {},
   "source": [
    "## 8) Pre-train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghapus label (autoencoder belajar rekonstruksi gambar)\n",
    "def strip_labels(ds):\n",
    "    \"\"\"Ubah (image, label) menjadi (image, image)\"\"\"\n",
    "    return ds.map(lambda x, y: (x, x))\n",
    "\n",
    "print(\"Mulai pre-training Autoencoder\")\n",
    "\n",
    "# Training autoencoder dengan ModelCheckpoint\n",
    "history_ae = autoencoder.fit(\n",
    "    strip_labels(train_ds),\n",
    "    validation_data=strip_labels(val_ds),\n",
    "    epochs=500,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=4,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            'autoencoder_best.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nPre-training Autoencoder selesai!\")\n",
    "print(\"Model terbaik disimpan: autoencoder_best.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c66a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot grafik loss autoencoder\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot Training & Validation Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_ae.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history_ae.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.title('Autoencoder - Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot epoch terbaik\n",
    "best_epoch = history_ae.history['val_loss'].index(min(history_ae.history['val_loss']))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(['Training Loss', 'Validation Loss'], \n",
    "        [history_ae.history['loss'][best_epoch], history_ae.history['val_loss'][best_epoch]],\n",
    "        color=['#3498db', '#e74c3c'])\n",
    "plt.title(f'Loss pada Epoch Terbaik (Epoch {best_epoch + 1})', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('autoencoder_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGrafik disimpan: autoencoder_training.png\")\n",
    "print(f\"Epoch terbaik: {best_epoch + 1}\")\n",
    "print(f\"Best Validation Loss: {min(history_ae.history['val_loss']):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713a3fcc",
   "metadata": {},
   "source": [
    "## 9) Keras Tuner ‚Äî cari LR terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db05e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "def build_model_for_tuning(hp: kt.HyperParameters):\n",
    "    \"\"\"\n",
    "    Fungsi untuk Keras Tuner mencari hyperparameter terbaik\n",
    "    Hyperparameter yang dicari:\n",
    "    - Learning rate\n",
    "    - Dropout rate\n",
    "    - Jumlah units di dense layer\n",
    "    \"\"\"\n",
    "    # Hyperparameter yang akan dicari\n",
    "    lr = hp.Float('learning_rate', min_value=1e-5, max_value=5e-3, sampling='log')\n",
    "    dropout = hp.Choice('dropout', values=[0.2, 0.3, 0.4])\n",
    "    dense_units = hp.Choice('dense_units', values=[64, 128, 256])\n",
    "    \n",
    "    # Gunakan encoder yang sudah di-pretrain (dibekukan)\n",
    "    enc = build_encoder(IMG_SIZE + (3,), 128)\n",
    "    enc.set_weights(encoder.get_weights())\n",
    "    enc.trainable = False  # Freeze encoder\n",
    "    \n",
    "    # Bangun model klasifikasi\n",
    "    inputs = keras.Input(shape=IMG_SIZE + (3,))\n",
    "    \n",
    "    # Preprocessing\n",
    "    x = augment(inputs)\n",
    "    x = preprocess(x)\n",
    "    \n",
    "    # Extract features dengan encoder\n",
    "    z = enc(x)\n",
    "    \n",
    "    # Classifier head\n",
    "    x = layers.Dropout(dropout)(z)\n",
    "    x = layers.Dense(dense_units, activation='relu')(x)\n",
    "    out = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "    # Compile model\n",
    "    model = Model(inputs, out)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Setup Keras Tuner\n",
    "print(\"Memulai pencarian hyperparameter terbaik dengan Keras Tuner...\")\n",
    "print(\"Proses ini akan memakan waktu cukup lama...\\n\")\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    build_model_for_tuning,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=12,\n",
    "    factor=3,\n",
    "    directory='kt_tuning',\n",
    "    project_name='lr_search'\n",
    ")\n",
    "\n",
    "# Callback untuk early stopping\n",
    "stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=4,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Mulai pencarian\n",
    "tuner.search(train_ds, validation_data=val_ds, callbacks=[stop], verbose=1)\n",
    "\n",
    "# Ambil hyperparameter terbaik\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "print(\"\\nHyperparameter terbaik:\")\n",
    "print(f\"  - Learning Rate: {best_hp.get('learning_rate'):.6f}\")\n",
    "print(f\"  - Dropout      : {best_hp.get('dropout')}\")\n",
    "print(f\"  - Dense Units  : {best_hp.get('dense_units')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2622006",
   "metadata": {},
   "source": [
    "## 10) Train Stage-1 (encoder beku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e7d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil hyperparameter terbaik dari tuning\n",
    "best_lr = best_hp.get('learning_rate')\n",
    "best_dropout = best_hp.get('dropout')\n",
    "best_dense = best_hp.get('dense_units')\n",
    "\n",
    "print(\"Membangun model klasifikasi Stage-1 (encoder dibekukan)...\")\n",
    "\n",
    "# Bangun encoder (dibekukan, tidak dilatih)\n",
    "enc = build_encoder(IMG_SIZE + (3,), 128)\n",
    "enc.set_weights(encoder.get_weights())\n",
    "enc.trainable = False  # Freeze semua layer encoder\n",
    "\n",
    "# Bangun model klasifikasi lengkap\n",
    "inputs = keras.Input(shape=IMG_SIZE + (3,))\n",
    "\n",
    "# Data augmentation dan preprocessing\n",
    "x = augment(inputs)\n",
    "x = preprocess(x)\n",
    "\n",
    "# Extract features dengan encoder\n",
    "z = enc(x)\n",
    "\n",
    "# Classifier head\n",
    "x = layers.Dropout(best_dropout)(z)\n",
    "x = layers.Dense(best_dense, activation='relu')(x)\n",
    "out = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "# Model lengkap\n",
    "clf = Model(inputs, out, name='classifier_stage1')\n",
    "\n",
    "# Compile model\n",
    "clf.compile(\n",
    "    optimizer=keras.optimizers.Adam(best_lr),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Mulai training Stage-1...\")\n",
    "print(\"Training dengan encoder beku...\\n\")\n",
    "\n",
    "# Training Stage-1\n",
    "hist1 = clf.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=6,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            'clf_stage1_best.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Stage-1 selesai!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grafik training Stage-1\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(hist1.history['loss'], label='Training Loss', linewidth=2, color='#3498db')\n",
    "plt.plot(hist1.history['val_loss'], label='Validation Loss', linewidth=2, color='#e74c3c')\n",
    "plt.title('Stage-1: Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(hist1.history['accuracy'], label='Training Accuracy', linewidth=2, color='#2ecc71')\n",
    "plt.plot(hist1.history['val_accuracy'], label='Validation Accuracy', linewidth=2, color='#f39c12')\n",
    "plt.title('Stage-1: Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot hasil terbaik\n",
    "best_epoch_s1 = hist1.history['val_accuracy'].index(max(hist1.history['val_accuracy']))\n",
    "plt.subplot(1, 3, 3)\n",
    "metrics = ['Train Acc', 'Val Acc', 'Train Loss', 'Val Loss']\n",
    "values = [\n",
    "    hist1.history['accuracy'][best_epoch_s1],\n",
    "    hist1.history['val_accuracy'][best_epoch_s1],\n",
    "    hist1.history['loss'][best_epoch_s1],\n",
    "    hist1.history['val_loss'][best_epoch_s1]\n",
    "]\n",
    "colors = ['#2ecc71', '#f39c12', '#3498db', '#e74c3c']\n",
    "plt.bar(metrics, values, color=colors, alpha=0.8)\n",
    "plt.title(f'Metrics Terbaik (Epoch {best_epoch_s1 + 1})', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('stage1_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGrafik disimpan: stage1_training.png\")\n",
    "print(f\"Epoch terbaik: {best_epoch_s1 + 1}\")\n",
    "print(f\"Best Validation Accuracy: {max(hist1.history['val_accuracy']):.4f} ({max(hist1.history['val_accuracy'])*100:.2f}%)\")\n",
    "print(f\"Model terbaik disimpan: clf_stage1_best.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa9382b",
   "metadata": {},
   "source": [
    "## 11) Fine-tune Stage-2 (unfreeze sebagian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf492c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unfreeze 6 layer terakhir encoder untuk fine-tuning...\")\n",
    "\n",
    "# Unfreeze 6 layer terakhir encoder\n",
    "for layer in enc.layers[-6:]:\n",
    "    if hasattr(layer, 'trainable'):\n",
    "        layer.trainable = True\n",
    "\n",
    "# Bangun ulang model dengan encoder yang sudah di-unfreeze\n",
    "inputs2 = keras.Input(shape=IMG_SIZE + (3,))\n",
    "\n",
    "# Data augmentation dan preprocessing\n",
    "x = augment(inputs2)\n",
    "x = preprocess(x)\n",
    "\n",
    "# Extract features dengan encoder (sebagian trainable)\n",
    "z = enc(x)\n",
    "\n",
    "# Classifier head\n",
    "x = layers.Dropout(best_dropout)(z)\n",
    "x = layers.Dense(best_dense, activation='relu')(x)\n",
    "out = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "# Model fine-tuned\n",
    "clf_ft = Model(inputs2, out, name='classifier_stage2')\n",
    "\n",
    "# Learning rate untuk fine-tuning (lebih kecil)\n",
    "ft_lr = max(best_lr / 5.0, 1e-5)\n",
    "\n",
    "# Compile model\n",
    "clf_ft.compile(\n",
    "    optimizer=keras.optimizers.Adam(ft_lr),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Model siap dengan LR fine-tuning: {ft_lr:.6f}\\n\")\n",
    "print(\"Mulai training Stage-2 (Fine-tuning)...\")\n",
    "print(\"Training dengan encoder sebagian trainable...\\n\")\n",
    "\n",
    "# Training Stage-2\n",
    "hist2 = clf_ft.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=6,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            'clf_stage2_finetuned.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nFine-tuning Stage-2 selesai!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grafik training Stage-2\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(hist2.history['loss'], label='Training Loss', linewidth=2, color='#3498db')\n",
    "plt.plot(hist2.history['val_loss'], label='Validation Loss', linewidth=2, color='#e74c3c')\n",
    "plt.title('Stage-2 (Fine-tuning): Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(hist2.history['accuracy'], label='Training Accuracy', linewidth=2, color='#2ecc71')\n",
    "plt.plot(hist2.history['val_accuracy'], label='Validation Accuracy', linewidth=2, color='#f39c12')\n",
    "plt.title('Stage-2 (Fine-tuning): Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot hasil terbaik\n",
    "best_epoch_s2 = hist2.history['val_accuracy'].index(max(hist2.history['val_accuracy']))\n",
    "plt.subplot(1, 3, 3)\n",
    "metrics = ['Train Acc', 'Val Acc', 'Train Loss', 'Val Loss']\n",
    "values = [\n",
    "    hist2.history['accuracy'][best_epoch_s2],\n",
    "    hist2.history['val_accuracy'][best_epoch_s2],\n",
    "    hist2.history['loss'][best_epoch_s2],\n",
    "    hist2.history['val_loss'][best_epoch_s2]\n",
    "]\n",
    "colors = ['#2ecc71', '#f39c12', '#3498db', '#e74c3c']\n",
    "plt.bar(metrics, values, color=colors, alpha=0.8)\n",
    "plt.title(f'Metrics Terbaik (Epoch {best_epoch_s2 + 1})', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('stage2_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGrafik disimpan: stage2_training.png\")\n",
    "print(f\"Epoch terbaik: {best_epoch_s2 + 1}\")\n",
    "print(f\"Best Validation Accuracy: {max(hist2.history['val_accuracy']):.4f} ({max(hist2.history['val_accuracy'])*100:.2f}%)\")\n",
    "print(f\"Model terbaik disimpan: clf_stage2_finetuned.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08be96",
   "metadata": {},
   "source": [
    "## 12) Evaluasi & ringkasan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d94d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perbandingan Stage-1 vs Stage-2\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# Perbandingan Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(hist1.history['val_loss'], label='Stage-1 (Frozen)', linewidth=2, color='#3498db', marker='o', markersize=4)\n",
    "plt.plot(hist2.history['val_loss'], label='Stage-2 (Fine-tuned)', linewidth=2, color='#e74c3c', marker='s', markersize=4)\n",
    "plt.title('Perbandingan Validation Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Perbandingan Accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(hist1.history['val_accuracy'], label='Stage-1 (Frozen)', linewidth=2, color='#2ecc71', marker='o', markersize=4)\n",
    "plt.plot(hist2.history['val_accuracy'], label='Stage-2 (Fine-tuned)', linewidth=2, color='#f39c12', marker='s', markersize=4)\n",
    "plt.title('Perbandingan Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Bar chart perbandingan hasil akhir\n",
    "plt.subplot(1, 3, 3)\n",
    "categories = ['Val Accuracy', 'Val Loss']\n",
    "stage1_results = [max(hist1.history['val_accuracy']), min(hist1.history['val_loss'])]\n",
    "stage2_results = [max(hist2.history['val_accuracy']), min(hist2.history['val_loss'])]\n",
    "\n",
    "x = range(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar([i - width/2 for i in x], stage1_results, width, label='Stage-1 (Frozen)', color='#3498db', alpha=0.8)\n",
    "plt.bar([i + width/2 for i in x], stage2_results, width, label='Stage-2 (Fine-tuned)', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "plt.title('Hasil Terbaik: Stage-1 vs Stage-2', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(x, categories)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Tambahkan nilai di atas bar\n",
    "for i, (v1, v2) in enumerate(zip(stage1_results, stage2_results)):\n",
    "    plt.text(i - width/2, v1, f'{v1:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    plt.text(i + width/2, v2, f'{v2:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('stage_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Ringkasan perbandingan\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä RINGKASAN PERBANDINGAN STAGE-1 vs STAGE-2\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Metric':<30} {'Stage-1':<15} {'Stage-2':<15} {'Improvement'}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "best_val_acc_s1 = max(hist1.history['val_accuracy'])\n",
    "best_val_acc_s2 = max(hist2.history['val_accuracy'])\n",
    "improvement_acc = ((best_val_acc_s2 - best_val_acc_s1) / best_val_acc_s1) * 100\n",
    "\n",
    "print(f\"{'Best Validation Accuracy':<30} {best_val_acc_s1:.4f}        {best_val_acc_s2:.4f}        {improvement_acc:+.2f}%\")\n",
    "\n",
    "best_val_loss_s1 = min(hist1.history['val_loss'])\n",
    "best_val_loss_s2 = min(hist2.history['val_loss'])\n",
    "improvement_loss = ((best_val_loss_s2 - best_val_loss_s1) / best_val_loss_s1) * 100\n",
    "\n",
    "print(f\"{'Best Validation Loss':<30} {best_val_loss_s1:.4f}        {best_val_loss_s2:.4f}        {improvement_loss:+.2f}%\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "if best_val_acc_s2 > best_val_acc_s1:\n",
    "    print(\"‚úì Fine-tuning meningkatkan performa model!\")\n",
    "else:\n",
    "    print(\"‚ö† Fine-tuning tidak meningkatkan performa (overfitting?)\")\n",
    "    \n",
    "print(\"\\nüìä Grafik perbandingan disimpan: stage_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab1869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä EVALUASI MODEL FINAL\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚è≥ Evaluasi model pada validation set...\\n\")\n",
    "\n",
    "# Evaluasi model\n",
    "res = clf_ft.evaluate(val_ds, return_dict=True)\n",
    "\n",
    "print(\"\\n‚úì Hasil evaluasi model final:\")\n",
    "print(f\"  - Loss    : {res['loss']:.4f}\")\n",
    "print(f\"  - Accuracy: {res['accuracy']:.4f} ({res['accuracy']*100:.2f}%)\")\n",
    "\n",
    "# Buat ringkasan training\n",
    "summary = {\n",
    "    'timestamp': datetime.datetime.now().isoformat(),\n",
    "    'num_classes': int(len(classes)),\n",
    "    'class_names': classes,\n",
    "    'img_size': IMG_SIZE,\n",
    "    'batch_size': int(32),\n",
    "    'best_hyperparameters': {\n",
    "        'learning_rate': float(best_lr),\n",
    "        'dropout': float(best_dropout) if isinstance(best_dropout, (int, float)) else best_dropout,\n",
    "        'dense_units': int(best_dense)\n",
    "    },\n",
    "    'fine_tuning_lr': float(ft_lr),\n",
    "    'autoencoder': {\n",
    "        'best_epoch': int(best_epoch + 1),\n",
    "        'best_val_loss': float(min(history_ae.history['val_loss']))\n",
    "    },\n",
    "    'stage1_frozen': {\n",
    "        'best_epoch': int(best_epoch_s1 + 1),\n",
    "        'best_val_accuracy': float(max(hist1.history['val_accuracy'])),\n",
    "        'best_val_loss': float(min(hist1.history['val_loss']))\n",
    "    },\n",
    "    'stage2_finetuned': {\n",
    "        'best_epoch': int(best_epoch_s2 + 1),\n",
    "        'best_val_accuracy': float(max(hist2.history['val_accuracy'])),\n",
    "        'best_val_loss': float(min(hist2.history['val_loss']))\n",
    "    },\n",
    "    'final_validation_metrics': res\n",
    "}\n",
    "\n",
    "# Simpan ringkasan ke file JSON\n",
    "with open('training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úì Ringkasan training disimpan: training_summary.json\")\n",
    "\n",
    "# Daftar file yang disimpan\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üíæ FILE YANG DISIMPAN:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìÅ Model:\")\n",
    "print(\"  - autoencoder_best.keras\")\n",
    "print(\"  - clf_stage1_best.keras\")\n",
    "print(\"  - clf_stage2_finetuned.keras (Model Final)\")\n",
    "\n",
    "print(\"\\nüìä Grafik:\")\n",
    "print(\"  - autoencoder_training.png\")\n",
    "print(\"  - stage1_training.png\")\n",
    "print(\"  - stage2_training.png\")\n",
    "print(\"  - stage_comparison.png\")\n",
    "\n",
    "print(\"\\nüìÑ Laporan:\")\n",
    "print(\"  - training_summary.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ SEMUA PROSES SELESAI!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
